{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>f</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.876300</td>\n",
       "      <td>0.087160</td>\n",
       "      <td>0.678294</td>\n",
       "      <td>1.741426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.506543</td>\n",
       "      <td>0.427771</td>\n",
       "      <td>0.117943</td>\n",
       "      <td>1.028571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.469181</td>\n",
       "      <td>0.208947</td>\n",
       "      <td>0.862431</td>\n",
       "      <td>1.539668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.997632</td>\n",
       "      <td>0.038999</td>\n",
       "      <td>0.628393</td>\n",
       "      <td>1.821145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.680144</td>\n",
       "      <td>0.432733</td>\n",
       "      <td>0.571025</td>\n",
       "      <td>1.691445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569</th>\n",
       "      <td>0.213704</td>\n",
       "      <td>0.183989</td>\n",
       "      <td>0.577722</td>\n",
       "      <td>1.052330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>0.727660</td>\n",
       "      <td>0.021602</td>\n",
       "      <td>0.820556</td>\n",
       "      <td>1.497021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571</th>\n",
       "      <td>0.452470</td>\n",
       "      <td>0.848223</td>\n",
       "      <td>0.896740</td>\n",
       "      <td>2.022459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>0.470553</td>\n",
       "      <td>0.443268</td>\n",
       "      <td>0.204643</td>\n",
       "      <td>1.091847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573</th>\n",
       "      <td>0.354489</td>\n",
       "      <td>0.567065</td>\n",
       "      <td>0.781405</td>\n",
       "      <td>1.660105</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>574 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            x         y         z         f\n",
       "0    0.876300  0.087160  0.678294  1.741426\n",
       "1    0.506543  0.427771  0.117943  1.028571\n",
       "2    0.469181  0.208947  0.862431  1.539668\n",
       "3    0.997632  0.038999  0.628393  1.821145\n",
       "4    0.680144  0.432733  0.571025  1.691445\n",
       "..        ...       ...       ...       ...\n",
       "569  0.213704  0.183989  0.577722  1.052330\n",
       "570  0.727660  0.021602  0.820556  1.497021\n",
       "571  0.452470  0.848223  0.896740  2.022459\n",
       "572  0.470553  0.443268  0.204643  1.091847\n",
       "573  0.354489  0.567065  0.781405  1.660105\n",
       "\n",
       "[574 rows x 4 columns]"
      ]
     },
     "execution_count": 586,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"result_testdata.csv\", delimiter=\",\")\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build the network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/develop-your-first-neural-network-with-pytorch-step-by-step/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch import autograd\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,:3]\n",
    "y = df.iloc[:,3]\n",
    "# y = df.iloc[:,:3].sum(axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(X.values.astype(np.float32), requires_grad=True)\n",
    "y = torch.tensor(y, dtype=torch.float32).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([574, 3])"
      ]
     },
     "execution_count": 590,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([574, 1])"
      ]
     },
     "execution_count": 591,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PimaClassifier(\n",
      "  (hidden1): Linear(in_features=3, out_features=160, bias=True)\n",
      "  (act1): ReLU()\n",
      "  (hidden2): Linear(in_features=160, out_features=80, bias=True)\n",
      "  (act2): ReLU()\n",
      "  (output): Linear(in_features=80, out_features=16, bias=True)\n",
      "  (act_output): Linear(in_features=16, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class PimaClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden1 = nn.Linear(3, 160)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.hidden2 = nn.Linear(160, 80)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.output = nn.Linear(80, 16)\n",
    "        self.act_output =  nn.Linear(16, 1)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.hidden1(x))\n",
    "        x = self.act2(self.hidden2(x))\n",
    "        x = self.act_output(self.output(x))\n",
    "        return x\n",
    " \n",
    "model = PimaClassifier()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.7414],\n",
       "        [1.0286],\n",
       "        [1.5397],\n",
       "        [1.8211],\n",
       "        [1.6914],\n",
       "        [1.4429],\n",
       "        [1.6620],\n",
       "        [1.2481],\n",
       "        [0.7386],\n",
       "        [1.8292],\n",
       "        [2.2020],\n",
       "        [1.5843],\n",
       "        [2.3136],\n",
       "        [1.1235],\n",
       "        [1.4134],\n",
       "        [1.4076],\n",
       "        [0.6883],\n",
       "        [2.5799],\n",
       "        [1.4953],\n",
       "        [1.5863],\n",
       "        [1.2879],\n",
       "        [1.1798],\n",
       "        [2.1594],\n",
       "        [1.7896],\n",
       "        [1.9594],\n",
       "        [1.9774],\n",
       "        [1.6865],\n",
       "        [2.3089],\n",
       "        [1.2882],\n",
       "        [1.0681],\n",
       "        [1.2656],\n",
       "        [1.2819],\n",
       "        [1.3000],\n",
       "        [1.1343],\n",
       "        [2.1116],\n",
       "        [0.8379],\n",
       "        [0.5024],\n",
       "        [1.1742],\n",
       "        [2.2438],\n",
       "        [0.7275],\n",
       "        [1.0485],\n",
       "        [1.3241],\n",
       "        [2.0168],\n",
       "        [1.8547],\n",
       "        [0.9294],\n",
       "        [2.1854],\n",
       "        [0.8303],\n",
       "        [0.9071],\n",
       "        [0.7578],\n",
       "        [1.0846],\n",
       "        [1.8044],\n",
       "        [0.7077],\n",
       "        [0.5011],\n",
       "        [2.0659],\n",
       "        [1.9117],\n",
       "        [1.1372],\n",
       "        [1.3982],\n",
       "        [1.9643],\n",
       "        [1.8333],\n",
       "        [1.3306],\n",
       "        [2.5951],\n",
       "        [1.8722],\n",
       "        [1.2202],\n",
       "        [1.4697],\n",
       "        [1.1341],\n",
       "        [1.2572],\n",
       "        [1.1613],\n",
       "        [1.0131],\n",
       "        [0.8965],\n",
       "        [1.0548],\n",
       "        [1.1379],\n",
       "        [1.2558],\n",
       "        [0.7169],\n",
       "        [0.9719],\n",
       "        [1.5199],\n",
       "        [0.6191],\n",
       "        [0.7020],\n",
       "        [1.9489],\n",
       "        [1.6735],\n",
       "        [1.1555],\n",
       "        [0.8784],\n",
       "        [1.9405],\n",
       "        [2.3477],\n",
       "        [2.3362],\n",
       "        [1.7829],\n",
       "        [1.5797],\n",
       "        [0.5727],\n",
       "        [1.6463],\n",
       "        [1.4770],\n",
       "        [0.3653],\n",
       "        [1.4705],\n",
       "        [0.2856],\n",
       "        [1.1631],\n",
       "        [1.4602],\n",
       "        [2.4609],\n",
       "        [2.1844],\n",
       "        [1.7703],\n",
       "        [1.0340],\n",
       "        [1.9355],\n",
       "        [1.4865],\n",
       "        [1.5368],\n",
       "        [1.1007],\n",
       "        [1.6297],\n",
       "        [2.7130],\n",
       "        [1.2975],\n",
       "        [1.6860],\n",
       "        [2.0929],\n",
       "        [1.7466],\n",
       "        [1.1597],\n",
       "        [1.5485],\n",
       "        [1.5659],\n",
       "        [1.2261],\n",
       "        [0.3754],\n",
       "        [1.7743],\n",
       "        [2.0949],\n",
       "        [1.3960],\n",
       "        [2.1996],\n",
       "        [1.3337],\n",
       "        [1.6643],\n",
       "        [0.8709],\n",
       "        [1.6194],\n",
       "        [1.7516],\n",
       "        [1.0334],\n",
       "        [1.4973],\n",
       "        [0.8182],\n",
       "        [0.4929],\n",
       "        [1.2659],\n",
       "        [1.3580],\n",
       "        [1.0453],\n",
       "        [2.0480],\n",
       "        [2.0299],\n",
       "        [2.1069],\n",
       "        [1.3334],\n",
       "        [1.9669],\n",
       "        [0.9487],\n",
       "        [1.2207],\n",
       "        [2.0667],\n",
       "        [1.8372],\n",
       "        [1.3999],\n",
       "        [1.0274],\n",
       "        [1.0646],\n",
       "        [2.3038],\n",
       "        [1.8874],\n",
       "        [1.3914],\n",
       "        [0.9913],\n",
       "        [0.8216],\n",
       "        [0.5369],\n",
       "        [1.2766],\n",
       "        [1.1750],\n",
       "        [1.7370],\n",
       "        [2.0425],\n",
       "        [2.5337],\n",
       "        [1.4773],\n",
       "        [1.9850],\n",
       "        [1.8917],\n",
       "        [1.6683],\n",
       "        [1.7443],\n",
       "        [1.6383],\n",
       "        [1.1354],\n",
       "        [2.2261],\n",
       "        [1.1171],\n",
       "        [2.0816],\n",
       "        [1.9025],\n",
       "        [1.3127],\n",
       "        [1.0200],\n",
       "        [1.4606],\n",
       "        [0.8533],\n",
       "        [1.8757],\n",
       "        [1.7689],\n",
       "        [1.5133],\n",
       "        [1.4055],\n",
       "        [1.9829],\n",
       "        [2.3076],\n",
       "        [1.8684],\n",
       "        [2.1190],\n",
       "        [0.7483],\n",
       "        [1.8104],\n",
       "        [1.1520],\n",
       "        [1.3955],\n",
       "        [1.5062],\n",
       "        [1.3353],\n",
       "        [0.9996],\n",
       "        [1.1054],\n",
       "        [1.4497],\n",
       "        [1.8614],\n",
       "        [1.5857],\n",
       "        [1.3318],\n",
       "        [2.3084],\n",
       "        [1.3200],\n",
       "        [1.0597],\n",
       "        [2.4273],\n",
       "        [1.6350],\n",
       "        [2.3508],\n",
       "        [1.6547],\n",
       "        [1.8207],\n",
       "        [1.5167],\n",
       "        [1.7182],\n",
       "        [1.3616],\n",
       "        [1.4919],\n",
       "        [1.1847],\n",
       "        [1.2835],\n",
       "        [0.3253],\n",
       "        [2.1737],\n",
       "        [2.3529],\n",
       "        [1.5505],\n",
       "        [1.9483],\n",
       "        [2.2909],\n",
       "        [1.9976],\n",
       "        [1.9615],\n",
       "        [1.1268],\n",
       "        [0.9649],\n",
       "        [1.6117],\n",
       "        [1.3424],\n",
       "        [1.7981],\n",
       "        [1.3005],\n",
       "        [0.9035],\n",
       "        [1.5751],\n",
       "        [0.6964],\n",
       "        [0.9713],\n",
       "        [2.5573],\n",
       "        [1.3676],\n",
       "        [0.9797],\n",
       "        [1.8241],\n",
       "        [1.8032],\n",
       "        [1.0474],\n",
       "        [1.1925],\n",
       "        [1.0239],\n",
       "        [1.4746],\n",
       "        [2.2149],\n",
       "        [1.9024],\n",
       "        [1.5133],\n",
       "        [1.2193],\n",
       "        [1.9943],\n",
       "        [1.3071],\n",
       "        [1.3067],\n",
       "        [2.1318],\n",
       "        [2.2120],\n",
       "        [1.7200],\n",
       "        [2.2805],\n",
       "        [1.6316],\n",
       "        [1.7140],\n",
       "        [2.4470],\n",
       "        [1.6349],\n",
       "        [1.4288],\n",
       "        [1.4020],\n",
       "        [1.9509],\n",
       "        [1.8820],\n",
       "        [0.6745],\n",
       "        [0.9179],\n",
       "        [2.1565],\n",
       "        [1.8474],\n",
       "        [1.5169],\n",
       "        [1.0458],\n",
       "        [1.0036],\n",
       "        [1.7624],\n",
       "        [0.6379],\n",
       "        [1.0009],\n",
       "        [1.3280],\n",
       "        [1.8822],\n",
       "        [1.1125],\n",
       "        [1.0325],\n",
       "        [1.4002],\n",
       "        [1.0740],\n",
       "        [2.1555],\n",
       "        [1.4162],\n",
       "        [1.0384],\n",
       "        [0.9151],\n",
       "        [2.1186],\n",
       "        [1.2934],\n",
       "        [1.2897],\n",
       "        [1.2285],\n",
       "        [1.5514],\n",
       "        [1.3458],\n",
       "        [1.0477],\n",
       "        [2.4231],\n",
       "        [1.8250],\n",
       "        [1.3881],\n",
       "        [1.7121],\n",
       "        [1.1508],\n",
       "        [1.0370],\n",
       "        [1.2942],\n",
       "        [1.6525],\n",
       "        [0.8698],\n",
       "        [1.0683],\n",
       "        [2.1482],\n",
       "        [1.4564],\n",
       "        [2.3553],\n",
       "        [1.3431],\n",
       "        [1.7008],\n",
       "        [1.1758],\n",
       "        [1.1942],\n",
       "        [1.5816],\n",
       "        [2.0453],\n",
       "        [0.9060],\n",
       "        [1.7148],\n",
       "        [1.1014],\n",
       "        [1.3302],\n",
       "        [1.6281],\n",
       "        [0.9973],\n",
       "        [2.0436],\n",
       "        [1.0161],\n",
       "        [2.2489],\n",
       "        [1.4909],\n",
       "        [1.6996],\n",
       "        [1.0669],\n",
       "        [0.3188],\n",
       "        [1.2869],\n",
       "        [0.9766],\n",
       "        [1.7814],\n",
       "        [2.0716],\n",
       "        [1.3252],\n",
       "        [1.7687],\n",
       "        [1.5510],\n",
       "        [1.6677],\n",
       "        [1.2751],\n",
       "        [1.5711],\n",
       "        [2.0185],\n",
       "        [1.4006],\n",
       "        [1.5957],\n",
       "        [2.2375],\n",
       "        [1.7887],\n",
       "        [0.7694],\n",
       "        [1.7619],\n",
       "        [2.6749],\n",
       "        [1.9523],\n",
       "        [1.2079],\n",
       "        [1.4044],\n",
       "        [1.6190],\n",
       "        [0.8562],\n",
       "        [1.6247],\n",
       "        [1.4778],\n",
       "        [1.4262],\n",
       "        [1.5810],\n",
       "        [1.4857],\n",
       "        [2.0935],\n",
       "        [2.1086],\n",
       "        [2.0061],\n",
       "        [1.6330],\n",
       "        [1.5355],\n",
       "        [0.9432],\n",
       "        [0.8642],\n",
       "        [1.9346],\n",
       "        [2.1699],\n",
       "        [0.5679],\n",
       "        [1.2699],\n",
       "        [1.4796],\n",
       "        [2.0241],\n",
       "        [1.6441],\n",
       "        [1.7203],\n",
       "        [1.7054],\n",
       "        [2.2958],\n",
       "        [1.7257],\n",
       "        [1.2781],\n",
       "        [1.0744],\n",
       "        [1.7690],\n",
       "        [1.1646],\n",
       "        [0.2877],\n",
       "        [1.1645],\n",
       "        [1.8365],\n",
       "        [2.0553],\n",
       "        [0.8909],\n",
       "        [0.7064],\n",
       "        [1.7378],\n",
       "        [1.3369],\n",
       "        [1.4007],\n",
       "        [0.5223],\n",
       "        [1.4388],\n",
       "        [2.2917],\n",
       "        [1.5255],\n",
       "        [1.3873],\n",
       "        [1.2400],\n",
       "        [1.8908],\n",
       "        [1.4702],\n",
       "        [1.4896],\n",
       "        [0.9865],\n",
       "        [1.9320],\n",
       "        [1.1982],\n",
       "        [2.1157],\n",
       "        [0.9150],\n",
       "        [0.8648],\n",
       "        [1.3884],\n",
       "        [1.3010],\n",
       "        [0.7904],\n",
       "        [2.5835],\n",
       "        [1.7480],\n",
       "        [1.1504],\n",
       "        [1.2512],\n",
       "        [2.6317],\n",
       "        [1.1798],\n",
       "        [1.3190],\n",
       "        [1.4830],\n",
       "        [2.2624],\n",
       "        [1.8847],\n",
       "        [1.3332],\n",
       "        [0.8886],\n",
       "        [0.9047],\n",
       "        [1.4192],\n",
       "        [1.1295],\n",
       "        [1.3479],\n",
       "        [1.1823],\n",
       "        [0.7677],\n",
       "        [1.3008],\n",
       "        [0.9713],\n",
       "        [1.9751],\n",
       "        [1.4089],\n",
       "        [1.4217],\n",
       "        [2.0162],\n",
       "        [1.7675],\n",
       "        [0.9674],\n",
       "        [1.5160],\n",
       "        [0.8837],\n",
       "        [1.7905],\n",
       "        [1.7232],\n",
       "        [1.4277],\n",
       "        [1.2454],\n",
       "        [0.7551],\n",
       "        [1.1995],\n",
       "        [0.6069],\n",
       "        [1.1471],\n",
       "        [1.4710],\n",
       "        [0.7671],\n",
       "        [1.2637],\n",
       "        [0.7543],\n",
       "        [1.2596],\n",
       "        [1.3511],\n",
       "        [1.0637],\n",
       "        [1.5692],\n",
       "        [1.6592],\n",
       "        [1.2820],\n",
       "        [1.2331],\n",
       "        [1.3653],\n",
       "        [1.2836],\n",
       "        [1.5382],\n",
       "        [1.2218],\n",
       "        [1.0665],\n",
       "        [1.7958],\n",
       "        [1.5952],\n",
       "        [2.8164],\n",
       "        [1.3594],\n",
       "        [1.3913],\n",
       "        [1.2458],\n",
       "        [0.8729],\n",
       "        [1.7545],\n",
       "        [1.2284],\n",
       "        [2.3174],\n",
       "        [0.9986],\n",
       "        [1.8996],\n",
       "        [0.6301],\n",
       "        [1.4037],\n",
       "        [1.6266],\n",
       "        [1.1764],\n",
       "        [1.5759],\n",
       "        [1.5842],\n",
       "        [0.4805],\n",
       "        [0.9187],\n",
       "        [1.1801],\n",
       "        [1.8985],\n",
       "        [1.6385],\n",
       "        [0.3060],\n",
       "        [1.3720],\n",
       "        [1.0583],\n",
       "        [1.8460],\n",
       "        [1.5319],\n",
       "        [1.2742],\n",
       "        [0.6871],\n",
       "        [1.3347],\n",
       "        [1.5697],\n",
       "        [0.8628],\n",
       "        [1.4621],\n",
       "        [1.4010],\n",
       "        [1.8748],\n",
       "        [1.8170],\n",
       "        [1.5247],\n",
       "        [1.2388],\n",
       "        [1.8172],\n",
       "        [1.8327],\n",
       "        [1.7000],\n",
       "        [1.4759],\n",
       "        [1.2489],\n",
       "        [1.4305],\n",
       "        [2.3896],\n",
       "        [2.0459],\n",
       "        [1.9277],\n",
       "        [1.1131],\n",
       "        [2.2076],\n",
       "        [1.8114],\n",
       "        [1.5967],\n",
       "        [1.0608],\n",
       "        [1.6082],\n",
       "        [1.5912],\n",
       "        [0.9847],\n",
       "        [1.7428],\n",
       "        [1.7232],\n",
       "        [1.6470],\n",
       "        [2.0760],\n",
       "        [1.0118],\n",
       "        [1.5248],\n",
       "        [1.3397],\n",
       "        [1.3754],\n",
       "        [0.7499],\n",
       "        [0.9338],\n",
       "        [0.5343],\n",
       "        [1.8682],\n",
       "        [1.9507],\n",
       "        [1.3807],\n",
       "        [2.0132],\n",
       "        [0.8552],\n",
       "        [0.9347],\n",
       "        [2.0798],\n",
       "        [2.0923],\n",
       "        [2.5409],\n",
       "        [1.7125],\n",
       "        [0.9818],\n",
       "        [2.0914],\n",
       "        [1.3406],\n",
       "        [1.9448],\n",
       "        [2.2875],\n",
       "        [0.9500],\n",
       "        [1.4064],\n",
       "        [1.5078],\n",
       "        [1.1555],\n",
       "        [1.9982],\n",
       "        [1.6147],\n",
       "        [1.9489],\n",
       "        [0.9699],\n",
       "        [1.3050],\n",
       "        [0.9971],\n",
       "        [1.4130],\n",
       "        [1.4507],\n",
       "        [2.4791],\n",
       "        [1.2482],\n",
       "        [1.8620],\n",
       "        [1.4222],\n",
       "        [1.5029],\n",
       "        [1.1870],\n",
       "        [1.7760],\n",
       "        [0.7660],\n",
       "        [1.4316],\n",
       "        [1.8082],\n",
       "        [1.2654],\n",
       "        [1.3055],\n",
       "        [1.8542],\n",
       "        [1.4134],\n",
       "        [1.8138],\n",
       "        [1.3708],\n",
       "        [1.2804],\n",
       "        [1.9835],\n",
       "        [2.5732],\n",
       "        [0.7053],\n",
       "        [1.5105],\n",
       "        [2.3203],\n",
       "        [1.2367],\n",
       "        [1.6702],\n",
       "        [1.1719],\n",
       "        [1.3052],\n",
       "        [2.1378],\n",
       "        [2.0424],\n",
       "        [1.5110],\n",
       "        [1.6117],\n",
       "        [1.1861],\n",
       "        [0.8863],\n",
       "        [1.7462],\n",
       "        [0.9519],\n",
       "        [1.6449],\n",
       "        [2.4097],\n",
       "        [1.0330],\n",
       "        [1.7762],\n",
       "        [1.4118],\n",
       "        [1.6095],\n",
       "        [1.0523],\n",
       "        [1.4970],\n",
       "        [2.0225],\n",
       "        [1.0918],\n",
       "        [1.6601]])"
      ]
     },
     "execution_count": 593,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 0, latest loss 0.03893100470304489\n",
      "Finished epoch 1, latest loss 0.00971320178359747\n",
      "Finished epoch 2, latest loss 0.005221494939178228\n",
      "Finished epoch 3, latest loss 0.004688470624387264\n",
      "Finished epoch 4, latest loss 0.0041638631373643875\n",
      "Finished epoch 5, latest loss 0.003627225523814559\n",
      "Finished epoch 6, latest loss 0.0026280786842107773\n",
      "Finished epoch 7, latest loss 0.001826498773880303\n",
      "Finished epoch 8, latest loss 0.0013636208605021238\n",
      "Finished epoch 9, latest loss 0.0010666153393685818\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.MSELoss()  # binary cross entropy\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "n_epochs = 10\n",
    "batch_size = 10\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        Xbatch = X[i:i+batch_size].clone()\n",
    "        Xbatch.requires_grad=True\n",
    "        y_pred = model(Xbatch)\n",
    "        f_x_t = autograd.grad(y_pred,Xbatch,torch.ones([Xbatch.shape[0], 1]), retain_graph=True, create_graph=True)[0] #first derivative\n",
    "        print(f_x_t)\n",
    "        ybatch = y[i:i+batch_size]\n",
    "        loss = loss_fn(y_pred, ybatch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Finished epoch {epoch}, latest loss {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 0.02 RMSE\n"
     ]
    }
   ],
   "source": [
    "# compute accuracy (no_grad is optional)\n",
    "# with torch.no_grad():\n",
    "#     y_pred = model(X)\n",
    "\n",
    "X.requires_grad_()\n",
    "X.retain_grad()\n",
    "y_pred = model(X)\n",
    "\n",
    "trainScore = math.sqrt(mean_squared_error(y_pred.detach().numpy(),y.detach().numpy()))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8763, 0.0872, 0.6783], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 596,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.7414])"
      ]
     },
     "execution_count": 597,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.7134], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 598,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0367, -0.0537, -0.0499],\n",
       "        [ 0.0605,  0.0329,  0.0393],\n",
       "        [ 0.0316,  0.0133,  0.0243],\n",
       "        ...,\n",
       "        [ 0.0196,  0.0099,  0.0098],\n",
       "        [ 0.1228,  0.0827,  0.1127],\n",
       "        [ 0.0626,  0.0519,  0.0711]])"
      ]
     },
     "execution_count": 599,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 600,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/xiaokeai/Documents/GitHub/PINNs/phyiscs_network/approximate_a_function_test.ipynb Cell 20\u001b[0m in \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/xiaokeai/Documents/GitHub/PINNs/phyiscs_network/approximate_a_function_test.ipynb#X33sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(model(X)\u001b[39m.\u001b[39;49mbackward())\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/autograd/__init__.py:190\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    186\u001b[0m inputs \u001b[39m=\u001b[39m (inputs,) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(inputs, torch\u001b[39m.\u001b[39mTensor) \u001b[39melse\u001b[39;00m \\\n\u001b[1;32m    187\u001b[0m     \u001b[39mtuple\u001b[39m(inputs) \u001b[39mif\u001b[39;00m inputs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mtuple\u001b[39m()\n\u001b[1;32m    189\u001b[0m grad_tensors_ \u001b[39m=\u001b[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001b[39mlen\u001b[39m(tensors))\n\u001b[0;32m--> 190\u001b[0m grad_tensors_ \u001b[39m=\u001b[39m _make_grads(tensors, grad_tensors_, is_grads_batched\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    191\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/autograd/__init__.py:85\u001b[0m, in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[39mif\u001b[39;00m out\u001b[39m.\u001b[39mrequires_grad:\n\u001b[1;32m     84\u001b[0m     \u001b[39mif\u001b[39;00m out\u001b[39m.\u001b[39mnumel() \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 85\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mgrad can be implicitly created only for scalar outputs\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     86\u001b[0m     new_grads\u001b[39m.\u001b[39mappend(torch\u001b[39m.\u001b[39mones_like(out, memory_format\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mpreserve_format))\n\u001b[1;32m     87\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "print(model(X).backward())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/xiaokeai/Documents/GitHub/PINNs/phyiscs_network/approximate_a_function_test.ipynb Cell 20\u001b[0m in \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/xiaokeai/Documents/GitHub/PINNs/phyiscs_network/approximate_a_function_test.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m grads \u001b[39m=\u001b[39m autograd\u001b[39m.\u001b[39;49mgrad(outputs\u001b[39m=\u001b[39;49mX, inputs\u001b[39m=\u001b[39;49my, create_graph\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/autograd/__init__.py:285\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    280\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39monly_inputs argument is deprecated and is ignored now \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m                   \u001b[39m\"\u001b[39m\u001b[39m(defaults to True). To accumulate gradient for other \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m                   \u001b[39m\"\u001b[39m\u001b[39mparts of the graph, please use torch.autograd.backward.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    284\u001b[0m grad_outputs_ \u001b[39m=\u001b[39m _tensor_or_tensors_to_tuple(grad_outputs, \u001b[39mlen\u001b[39m(t_outputs))\n\u001b[0;32m--> 285\u001b[0m grad_outputs_ \u001b[39m=\u001b[39m _make_grads(t_outputs, grad_outputs_, is_grads_batched\u001b[39m=\u001b[39;49mis_grads_batched)\n\u001b[1;32m    287\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    288\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/autograd/__init__.py:85\u001b[0m, in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[39mif\u001b[39;00m out\u001b[39m.\u001b[39mrequires_grad:\n\u001b[1;32m     84\u001b[0m     \u001b[39mif\u001b[39;00m out\u001b[39m.\u001b[39mnumel() \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 85\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mgrad can be implicitly created only for scalar outputs\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     86\u001b[0m     new_grads\u001b[39m.\u001b[39mappend(torch\u001b[39m.\u001b[39mones_like(out, memory_format\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mpreserve_format))\n\u001b[1;32m     87\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "grads = autograd.grad(outputs=y, inputs=X, torch.ones([X.shape[0], 1]),retain_graph=True, create_graph=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
